The findsign module extracts the parking sign from the image.  It
returns a sub image with as little noise as possible, however there
may be some non-letter objects in the image, such as bolts, arrows,
etc.

Simply doing edge detection + contours and looking for the top five or
so large contours that correlate well with the shape of a sign works
OK.  Note that edge detection works best if done before binarizing the
image.  We need to add some additional classifiers, such as checking
the noise inside the bounding rectangle.  E.g., we could down sample
and then upsample and compare the two images.

One more thought is to search for straight line segments in the image
(including slanted ones), and then look for the region with a
concentration of parallel and perpendicular segments close together.
Some other things may match, line power lines and windows, but this
might work decently well.  Sign edges and letters within should match,
even if the sign is tilted.  We'd need a good way to detect line
segments.  I'm not sure Hough transforms would work well, since they
will probably match a lot of noise in pictures with trees, fences,
etc.

The Python interface to OpenCV doesn't give access to CvChain, meaning
that we can't figure out the origin of the Freeman chain codes, and
hence can't figure out the bounding box for the contour.  Some
thoughts to work around this come to mind.  I tried generating both
the Freeman codes and point sequences from FindContours.  Assuming
they return the same contours in the same order, we could use them in
parallel to get all the info we need.  Another way would be to draw
the contour using DrawContours on a black image, and then use
BoundingBox.  Or we could modify the swig wrappers to return the
origin.  :\
