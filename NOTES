
To run any classifiers in the subdirectories, add common/ to the
python path.

General
=======

We also need to start working on words.  The clustering approach
worked promisingly before.

We may want to design some classifiers specifically for
differentiating between specific common letters.  E.g., if we think
something is a c, run simple classifiers to separate it from e and G.
These could be as simple as looking for pixels in specific areas.  It
would be cool if we had a learning algorithm that figured out the best
points to check for us.

We could also use the alternatives themselves to classify.  E.g., if
we see that P often matches O and Q, then if we see those letters high
in the rankings, it lends weight to the correct choice being a P.  Of
course, we need to avoid classifying O or Q as P just because they are
similar, but it basically gives us a clustering algorithm from which
we may be able to detect drastic misclassifications.  E.g., if we the
top rankings aren't in a known cluster, it indicates the guess is a
bad one.

One thing I've learned from this is that it's easier to classify
signals than to separate signals from noise.  =\

We should look for cheaper features.  The algorithms used here are
pretty slow, and we should be able to learn some cheap features that
separate the data well.

Hierarchy
=========

Death to edge detection!  Long live binarization!  The problem with
finding contours on an edge detected image is that you get both the
inside and the outside of the object, because a contour matches both
sides of the edge.  The is unnecessary and leads to poor results.  It
also seems like you get a lot more noise because there are many more
contours.  Binarization simplifies the image whereas edge detection
results in something more complicated and harder to work with.  We
need to be sure the letters are white, as contours only stick to white
things.  Converting the image to binary properly so that we can see
characters on either a light or dark background may present some
challenges, but it's working well for the most part now.

Combine
=======

Look into using better aggregation schemes, such as standardizing the
data before multiplying certainties, or going a whole new route and
try boosting.

kNN
===

Nearest neighbor might be a good way to separate letters from noise.
Simply generate a training set with features from both letters and
noise, and then the test would be to find the K nearest neighbors of a
test point, and use the number of letters divided by K as the
certainty of the classification.

ML
==

Machine learning using logistic regression is not performing too well.
It only classifies letters correctly about 30% of the time.  This may
just be a hard problem (separating signals from noise), but I'm
starting to suspect I'm just using poor features.  Symmetry is a
fairly tolerant measure, and some letters are nearly indistinguishable
from each other (e.g., I and O are both symmetric about every axis,
and S and Z are similar in their symmetry).  Perhaps we could succeed
more simply by finding more telling features.  What about derivatives
of intensity across rows (or columns) as we scan from top to bottom
(or left to right)?  Since noise typically has large derivatives, this
could be a useful feature.

Contours
========

Checking serif vs sans:

Classified 33/52 correctly (63.46%)
Average certainty for correct classification: 0.032 (min: 0.021)
Average certainty for incorrect classification: 0.025 (max: 0.029)
Incorrect classifications:
          Thought B was I (certainty: 0.024); alternatives were l, i, B
          Thought D was l (certainty: 0.025); alternatives were I, B, D
          Thought g was r (certainty: 0.023); alternatives were i, g, q
          Thought H was s (certainty: 0.027); alternatives were H, i, F
          Thought i was j (certainty: 0.026); alternatives were g, R, F
          Thought J was R (certainty: 0.026); alternatives were j, E, F
          Thought l was R (certainty: 0.024); alternatives were i, l, I
          Thought M was q (certainty: 0.024); alternatives were M, o, p
          Thought n was R (certainty: 0.027); alternatives were h, J, o
          Thought p was i (certainty: 0.024); alternatives were r, P, l
          Thought P was i (certainty: 0.025); alternatives were r, F, l
          Thought q was i (certainty: 0.025); alternatives were I, o, l
          Thought Q was o (certainty: 0.027); alternatives were O, Q, i
          Thought r was i (certainty: 0.023); alternatives were T, I, l
          Thought R was j (certainty: 0.029); alternatives were n, J, P
          Thought T was B (certainty: 0.026); alternatives were l, I, D
          Thought w was L (certainty: 0.023); alternatives were o, l, I
          Thought W was L (certainty: 0.024); alternatives were o, q, I
          Thought Y was r (certainty: 0.026); alternatives were Y, o, b

Results for sign1:

Classified 10/15 correctly (66.67%)
Average certainty for correct classification: 0.029 (min: 0.025)
Average certainty for incorrect classification: 0.028 (max: 0.032)
Incorrect classifications:
          Thought G was c (certainty: 0.032); alternatives were C, e, G
          Thought H was K (certainty: 0.025); alternatives were H, k, b
          Thought M was b (certainty: 0.030); alternatives were w, W, M
          Thought N was K (certainty: 0.028); alternatives were k, i, b
          Thought P was o (certainty: 0.026); alternatives were O, Q, i

Most results look fairly sane.  We can add second (encolsed) contour
checks which I think will improve some of these a bit.  We can still
check out some of the incorrect responses, but it seems like progress
is slowing down on this classifier, and it does a decent job.  Even
for those it doesn't classify correctly, the correct character is
often in the top four ranking.  This means we could match the word to
a dictionary and probably get good results.

Not sure if we still want to look into weighing correlation more than
error, etc.

Another observation is that when looking at plots of contour angles,
they often line up at the beginning, but start shifting out of phase
towards the end.  (Look into why cross-correlation lines the beginning
up more than the end.)  This can be true of letters with different
thickness, or with serifs, etc., when the permiter of one letter is
longer than the other.  An idea to mitigate this is to split one
contour angle vector into sections (say 10), and then doing a
correlation on each with the other letter contour.  The mean x values
for each segment should at least be increasing, although we can
tolerate some overlap or gaps.  Then average the correlations and
errors.

Templates
=========

Results for sign1:

Classified 11/16 correctly (68.75%)
Average certainty for correct classification: 0.028 (min: 0.027)
Average certainty for incorrect classification: 0.025 (max: 0.027)
Incorrect classifications:
          Thought H was I (certainty: 0.025); alternatives were R, B, a
          Thought K was I (certainty: 0.026); alternatives were K, k, R
          Thought R was I (certainty: 0.026); alternatives were R, P, B
          Thought R was I (certainty: 0.027); alternatives were B, R, P
          Thought s was l (certainty: 0.024); alternatives were i, I, M

For serif:

Classified 34/52 correctly (65.38%)
Average certainty for correct classification: 0.029 (min: 0.023)
Average certainty for incorrect classification: 0.028 (max: 0.037)
Incorrect classifications:
          Thought B was S (certainty: 0.024); alternatives were B, s, C
          Thought d was J (certainty: 0.026); alternatives were d, a, L
          Thought D was O (certainty: 0.025); alternatives were D, o, C
          Thought e was C (certainty: 0.028); alternatives were O, c, o
          Thought E was L (certainty: 0.025); alternatives were E, F, Z
          Thought g was V (certainty: 0.023); alternatives were y, Y, C
          Thought G was C (certainty: 0.027); alternatives were O, G, c
          Thought H was R (certainty: 0.023); alternatives were h, H, F
          Thought i was T (certainty: 0.032); alternatives were f, Y, t
          Thought I was T (certainty: 0.037); alternatives were f, t, Y
          Thought J was T (certainty: 0.030); alternatives were f, j, Y
          Thought l was T (certainty: 0.036); alternatives were f, t, Y
          Thought M was V (certainty: 0.027); alternatives were Y, y, v
          Thought R was k (certainty: 0.026); alternatives were X, L, K
          Thought u was J (certainty: 0.023); alternatives were c, o, d
          Thought U was L (certainty: 0.025); alternatives were U, C, O
          Thought v was Y (certainty: 0.033); alternatives were y, V, v
          Thought V was Y (certainty: 0.034); alternatives were y, V, v

This seems to actually work pretty well, except that everything
matches I/i/l.  Obviously this is just because those masks are
basically solid blocks of ones, so it's essentially about the best you
can do with ones alone.  The only reason the true letter does better
is it also gets values from the zeros, of which there are typically
far less.  Hence, an idea is to add extra weight to the zeros.  We can
do this by looking at the ratio of the total number of zeros in both
images to the total number of ones, and then weighting so that both ON
and OFF pixels have equal importance.  This seems reasonable for some
letters, but maybe not all, like for two Is.

A similar thought would be to subtract mismatched pixels so they
actually count against scoring.

Another idea to mitigate the problem of I/i/l domination is to just
treat these specially and not even test for them.  We'll need some
other strategy to observe them.

A more complicated approach to solving the I/i/l problem is to weigh
the inside of regions more than the outsides.  We could just do this
in a row-oriented fashion to begin with.  Look at the run length of
contiguous sections of ones or zeros in each row, and instead of
counting them all the same, assign weight to them with some
distribution, such as a gaussian.  Hence, if the masks don't exactly
line up, we should at least get more from the parts that do.  We need
to normalize so that the row values sum to the same thing for all
masks so that we don't introduce bias unfairly.
