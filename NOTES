
To run any classifiers in the subdirectories, add common/ to the
python path.

General
=======

We also need to start working on words.  The clustering approach
worked promisingly before.

We may want to design some classifiers specifically for
differentiating between specific common letters.  E.g., if we think
something is a c, run simple classifiers to separate it from e and G.
These could be as simple as looking for pixels in specific areas.  It
would be cool if we had a learning algorithm that figured out the best
points to check for us.

We could also use the alternatives themselves to classify.  E.g., if
we see that P often matches O and Q, then if we see those letters high
in the rankings, it lends weight to the correct choice being a P.  Of
course, we need to avoid classifying O or Q as P just because they are
similar, but it basically gives us a clustering algorithm from which
we may be able to detect drastic misclassifications.  E.g., if we the
top rankings aren't in a known cluster, it indicates the guess is a
bad one.

Contours
========

Checking serif vs sans:

Classified 33/52 correctly (63.46%)
Average certainty for correct classification: 0.032 (min: 0.021)
Average certainty for incorrect classification: 0.025 (max: 0.029)
Incorrect classifications:
          Thought B was I (certainty: 0.024); alternatives were l, i, B
          Thought D was l (certainty: 0.025); alternatives were I, B, D
          Thought g was r (certainty: 0.023); alternatives were i, g, q
          Thought H was s (certainty: 0.027); alternatives were H, i, F
          Thought i was j (certainty: 0.026); alternatives were g, R, F
          Thought J was R (certainty: 0.026); alternatives were j, E, F
          Thought l was R (certainty: 0.024); alternatives were i, l, I
          Thought M was q (certainty: 0.024); alternatives were M, o, p
          Thought n was R (certainty: 0.027); alternatives were h, J, o
          Thought p was i (certainty: 0.024); alternatives were r, P, l
          Thought P was i (certainty: 0.025); alternatives were r, F, l
          Thought q was i (certainty: 0.025); alternatives were I, o, l
          Thought Q was o (certainty: 0.027); alternatives were O, Q, i
          Thought r was i (certainty: 0.023); alternatives were T, I, l
          Thought R was j (certainty: 0.029); alternatives were n, J, P
          Thought T was B (certainty: 0.026); alternatives were l, I, D
          Thought w was L (certainty: 0.023); alternatives were o, l, I
          Thought W was L (certainty: 0.024); alternatives were o, q, I
          Thought Y was r (certainty: 0.026); alternatives were Y, o, b

Results for sign1:

Classified 10/15 correctly (66.67%)
Average certainty for correct classification: 0.029 (min: 0.025)
Average certainty for incorrect classification: 0.028 (max: 0.032)
Incorrect classifications:
          Thought G was c (certainty: 0.032); alternatives were C, e, G
          Thought H was K (certainty: 0.025); alternatives were H, k, b
          Thought M was b (certainty: 0.030); alternatives were w, W, M
          Thought N was K (certainty: 0.028); alternatives were k, i, b
          Thought P was o (certainty: 0.026); alternatives were O, Q, i

Most results look fairly sane.  We can add second (encolsed) contour
checks which I think will improve some of these a bit.  We can still
check out some of the incorrect responses, but it seems like progress
is slowing down on this classifier, and it does a decent job.  Even
for those it doesn't classify correctly, the correct character is
often in the top four ranking.  This means we could match the word to
a dictionary and probably get good results.

Not sure if we still want to look into weighing correlation more than
error, etc.

Another observation is that when looking at plots of contour angles,
they often line up at the beginning, but start shifting out of phase
towards the end.  (Look into why cross-correlation lines the beginning
up more than the end.)  This can be true of letters with different
thickness, or with serifs, etc., when the permiter of one letter is
longer than the other.  An idea to mitigate this is to split one
contour angle vector into sections (say 10), and then doing a
correlation on each with the other letter contour.  The mean x values
for each segment should at least be increasing, although we can
tolerate some overlap or gaps.  Then average the correlations and
errors.

Templates
=========

Results for sign1:

Classified 11/16 correctly (68.75%)
Average certainty for correct classification: 0.028 (min: 0.027)
Average certainty for incorrect classification: 0.025 (max: 0.027)
Incorrect classifications:
          Thought H was I (certainty: 0.025); alternatives were R, B, a
          Thought K was I (certainty: 0.026); alternatives were K, k, R
          Thought R was I (certainty: 0.026); alternatives were R, P, B
          Thought R was I (certainty: 0.027); alternatives were B, R, P
          Thought s was l (certainty: 0.024); alternatives were i, I, M

For serif:

Classified 34/52 correctly (65.38%)
Average certainty for correct classification: 0.029 (min: 0.023)
Average certainty for incorrect classification: 0.028 (max: 0.037)
Incorrect classifications:
          Thought B was S (certainty: 0.024); alternatives were B, s, C
          Thought d was J (certainty: 0.026); alternatives were d, a, L
          Thought D was O (certainty: 0.025); alternatives were D, o, C
          Thought e was C (certainty: 0.028); alternatives were O, c, o
          Thought E was L (certainty: 0.025); alternatives were E, F, Z
          Thought g was V (certainty: 0.023); alternatives were y, Y, C
          Thought G was C (certainty: 0.027); alternatives were O, G, c
          Thought H was R (certainty: 0.023); alternatives were h, H, F
          Thought i was T (certainty: 0.032); alternatives were f, Y, t
          Thought I was T (certainty: 0.037); alternatives were f, t, Y
          Thought J was T (certainty: 0.030); alternatives were f, j, Y
          Thought l was T (certainty: 0.036); alternatives were f, t, Y
          Thought M was V (certainty: 0.027); alternatives were Y, y, v
          Thought R was k (certainty: 0.026); alternatives were X, L, K
          Thought u was J (certainty: 0.023); alternatives were c, o, d
          Thought U was L (certainty: 0.025); alternatives were U, C, O
          Thought v was Y (certainty: 0.033); alternatives were y, V, v
          Thought V was Y (certainty: 0.034); alternatives were y, V, v

This seems to actually work pretty well, except that everything
matches I/i/l.  Obviously this is just because those masks are
basically solid blocks of ones, so it's essentially about the best you
can do with ones alone.  The only reason the true letter does better
is it also gets values from the zeros, of which there are typically
far less.  Hence, an idea is to add extra weight to the zeros.  We can
do this by looking at the ratio of the total number of zeros in both
images to the total number of ones, and then weighting so that both ON
and OFF pixels have equal importance.  This seems reasonable for some
letters, but maybe not all, like for two Is.

A similar thought would be to subtract mismatched pixels so they
actually count against scoring.

Another idea to mitigate the problem of I/i/l domination is to just
treat these specially and not even test for them.  We'll need some
other strategy to observe them.

A more complicated approach to solving the I/i/l problem is to weigh
the inside of regions more than the outsides.  We could just do this
in a row-oriented fashion to begin with.  Look at the run length of
contiguous sections of ones or zeros in each row, and instead of
counting them all the same, assign weight to them with some
distribution, such as a gaussian.  Hence, if the masks don't exactly
line up, we should at least get more from the parts that do.  We need
to normalize so that the row values sum to the same thing for all
masks so that we don't introduce bias unfairly.
